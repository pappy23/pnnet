#
# NetCache
#
class NetCache:
    """Contains list of network layers

    Is automatically generated from arbitrary topology"""

    def __init__(self):
        self._ok = False
        self.layers = []

    def is_ok(self):
        return self._ok

    def invalidate(self):
        self._ok = False

    def fix(self):
        self._ok = True

#
# Barrier
#
from threading import Condition

class Barrier:
    """Locks all Barrier.wait() calls until N threads call wait()

    It's a synchronization object inspired by boost::barrier
    Code taken form here: http://www.python.org/search/hypermail/python-1994q2/0608.html"""

    def __init__(self, threads):
        self.__threads = self.__remains = threads
        self.__cond = Condition()

    def wait(self):
        with self.__cond:
            self.__remains -= 1
            if self.__remains:
                self.__cond.wait()
            else:
                self.__remains = self.__threads
                self.__cond.notifyAll()

#
# Net
#
from multiprocessing import cpu_count
from threading import Thread
from Weight import Weight
from Link import Link
from Neuron import Neuron

class Net:
    """Container for all types of neurons

    Net concept is very simple: it holds only input neurons and provides simple API
    for (dis)connecting neurons. Also Net maintains internal NetCache object

    How to use:
        - create empty Net()
        - add some input neurons to it(neurons are NOT generated by Net, but provided by user)
            net.add_input_neurons(customNeuron1)
        - connect neurons into topology with Net.connect(neuron1, neuron2, Weight_object)
        - give some input to net (net.set_input([1,2,3]))
        - run it (see details below)
        - here you are - take your output (net.get_output)

    Running Net:
        Every time you call Net.run() method you ought to supply two arguments - reference to Runner object and boolean value
        Runner is a function that takes exactly two arguments - Neuron anf Net
        When network runs itself it uses it's cache (it's rebuild after each change in topology)
        to apply runner to every neuron, layer after layer. Neurons in same layer are fed to runner in parallel, depending
        on configured worker threads count. Runner gets current neurons address as first argument and Net.self as second argument.
        Runner shouldn't change supplied Net object! If only python could enforce read only access methods...
        There is a guarantee that one neuron is supplied to exactly one runner at a time, so runner can do everything  with
        his neuron %)
        Second argument to Net.run() is direction specifier. If True then net runs itself from input layer to output layer. Every
        next layer is processed only after all neurons in previous layer are processed. If direction is False, then net reverts
        it's cache and begins from output neurons

    Runners:
        While writing your own runner remember - you should be very careful, because a lot of runners may be executed in parallel
        on different neurons but on same Net object! When dealing woth neuron - remember: only Weight object may be shared between
        two or more links (it provides thread safe Weight.add_value() method)!
        """

    def __init__(self):
        """Creates empty net without any neurons

        Signature: f(None) -> None"""
        self.input_neurons = []
        self.worker_threads_count = cpu_count()
        self._cache = NetCache()

    def add_input_neuron(self, n):
        """Register supplied Neuron object as input neuron for this Net

        Signature: f(Neuron) -> None"""
        assert isinstance(n, Neuron)

        self._cache.invalidate()
        self.input_neurons.append(n)

    def remove_neuron(self, n):
        """Removes all connections to or from neuron and deletes it

        Signature: f(Neuron) -> None"""
        assert isinstance(n, Neuron)

        self._cache.invalidate()
        for link in n._links_in:
            self.disconnect(link.to(), n)
        for link in n._links_out:
            self.disconnect(n, link.to())
        try:
            self.input_neurons.remove(n)
        except ValueError: #This neuron isn't registered as input
            pass
        del n

    def connect(self, n_from, n_to, w = None, latency = 1):
        """Adds appropriate symmetric Links to two neurons

        For description of latency argument see documentation on Net._update_cache()
        Looks like this functions may be defined outside of class Net,
        but if we change connections in net - we break topology so connect
        method is included in class Net only to invalidate cache

        Signature: f(Neuron, Neuron, Weight=None, int=1) -> None"""
        assert isinstance(n_from, Neuron)
        assert isinstance(n_to, Neuron)
        assert isinstance(w, (type(None), Weight))

        self._cache.invalidate()
        if w is None:
            w = Weight()
        n_from._links_out.append(Link(n_to, w, latency))
        n_to._links_in.append(Link(n_from, w, latency))

    def disconnect(self, n_from, n_to):
        """Disconnects two neurons. See explanations for Net.connect()

        Signature: f(Neuron, Neuron) -> None"""
        assert isinstance(n_from, Neuron)
        assert isinstance(n_to, Neuron)

        self._cache.invalidate()
        n_from._links_out = [link for link in n_from._links_out if link.to() is not n_to]
        n_to._links_in = [link for link in n_to._links_in if link.to() is not n_from]

    def set_input(self, values):
        """Assign supplied values to Neuron.input fields of net's input neurons

        Signature: f(list[float]) -> None"""
        assert isinstance(values, list)
        assert len(values) == len(self.input_neurons)

        for i in range(len(self.input_neurons)):
            assert type(values[i]) in (int, float)
            self.input_neurons[i].input = values[i]

    def get_output(self):
        """Get network output

        Signature: f(None) -> list[float]"""
        if not self._cache.is_ok():
            self._update_cache()
        if len(self._cache.layers):
            return map(lambda n: n.output, self._cache.layers[-1])
        else:
            return []

    def run(self, runner = lambda x, y: x.run(), dir = True):
        """Run network with specified runner and direction
        If direction is True then run in feedforward propagation mode
        In case if direction is False then perform backpropagation run

        Signature: f(function, bool) -> None"""
        def worker(id):
            """Inline definition of main loop function for worker thread

            Function takes 'id' of current thread to equally share load between threads

            Signature: f(int) -> None"""
            for layer in self._cache.layers:
                # Call runner only for specific neurons in current layer
                for i in range(id, len(layer), self.worker_threads_count):
                    runner(layer[i], self)
                # Wait for all threads to finish processing current layer
                """barrier.wait()"""

        assert callable(runner)
        assert isinstance(dir, bool)

        if not self._cache.is_ok():
            self.__update_cache()
        # This is magic behind backpropagation :)
        if not dir:
            self._cache.layers.reverse()
        thread_pool = [] # Remember all created threads
        barrier = Barrier(self.worker_threads_count)
        for i in range(self.worker_threads_count):
            t = Thread(target = worker, args = [i])
            t.daemon = True
            t.start()
            thread_pool.append(t)
        #DEBUG
        for i in range(self.worker_threads_count, 10):
            t = Thread()
            t.daemon = True
            t.start()
            thread_pool.append(t)
        # Wait for threads to finish
        for t in thread_pool:
            t.join()
        # "Repair" cache
        if not dir:
            self._cache.layers.reverse()

    def __update_cache(self):
        """Rebuild internal cache on demand

        It's not difficult to see that Net allows for really arbitrary topologies, but
        internally every topology is presented as list of layers even if there are recursive
        links inside same layer, between layers and loopbacks over same neuron.
        This flexibility is accomplished by __update_cache algorithm. Read code below if you
        really want to understand the magic behind the scenes

        Short comment from old C++ version of code:
        We maintain 'hops' attribute for every neuron. Hop signifies distance from input layer
        Consider following topology (hops are in brackets):
          +(0)    <-- output          +(0)             +(3)    <-- front(3)
         / \                         / \              / \
        +(0)+(0)         ======>    +(0)+(0)  ====>  +(2)+(2)  <-- front(2)
         \ /           iteration1    \ /        N     \ /
          +(0)  <-- input             +(1)             +(1)    <-- front on iteration1
          For current neuron (C) and other neuron (T), hops might be:
          T = 0  - T-neuron is fresh. We will set T=C+1
          T = N, N < C - it is recurrent link. Don't touch T and don't place it to next_layer
          T = C  - it is recursive topology(recursion in same layer)
          T = C, but t=c  - it is recurrent link over 1 neuron
          T = C + 1 - T already handled. We should add T to next_layer
          T > C + 1 - latency to T is > 1. Don't touch T.
        Latency of link is used for building of shortcut topologies - where output layer
        can receive not only from previous layer but also from other input and hidden layers
        Misuse of latency attribute can lead to not usable topology
        All above considerations are summarized in checks. Read code.

        Signature: f(None) -> None"""
        self._cache.layers = []
        # For all input neurons hopcount is 1
        hops = dict(map(lambda x: (x, 1), self.input_neurons))
        # We work with sets to remove duplicates
        layer = set(self.input_neurons)
        while len(layer):
            self._cache.layers.append(list(layer)) # Built layer is appended to cache
            next_layer = set()
            for n in layer: # Get neurons to which current layer connects
                for link in n._links_out:
                    if not link.to() in hops:
                        hops[link.to()] = hops[n] + link.latency()
                    if hops[link.to()] == hops[n] + 1:
                        next_layer.add(link.to())
                layer = next_layer

        self._cache.fix()

