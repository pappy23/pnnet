#summary Идеи по проекту

Идея такая.
На винте лежит сеть. Сеть - это два массива.
Массив нейронов, массив весов.

Нейрон:
-id
-вектор Связей нейрона
-ссылка на класс - функцию активации

Связь:
-направление (in/out)
-к какому нейрону идет связь
-ссылка на Вес
-номер процесса, который обслуживает нейрон (к которому идет связь)

Вес:
-счетчик использований
-значение веса

Функция активации:
{статический класс}
-функция
-производная
-всякие параметры и т.п.

Сеть ослуживается несколькими рабочими процессами MPI.
каждому процессу отводится свой сегмент сети.

Алгоритм:
1)процесс №0 читает файл с сетью.
2)смотрит на каждый нейрон и отдает его определенному процессу на обработку
3)каждый процесс такие нейроны накапливает у себя в векторе
4)если рабочий процесс видит, что к данному нейрону ведут связи от других процессов - считаем
этот нейрон "входным" и помещаем в рабочую очередь.
5)все. все нейроны поделили между процессами
6)процессы начинают работать
7)нейроны, которые являются входными или выходными для всей сети - обслуживаются процессом №0
8)каждый процесс начинает работу с рабочей очереди.
9)процесс последовательно выполняет с каждым элементом рабочей очереди след. действия
9.1)смотрим, какие связи ведут к данному нейрону. Если связь ведет к другому процессу - пытаемся запросить
у другого процесса значение активации нужного нейрона. Если связь ведет к "своему" нейрону, то просто считываем его
значение активации и накапливаем в локальном рецептивном поле данного нейрона.
9.2)смотрим какие связи выходят из данного нейрона. Если они ведут к "своему" нейрону, добавляем этот нейрон
в рабочую очередь. Если это нейрон другого процесса - ничего больше не делаем, он и так уже есть в рабочей очереди
другого процесса

АЛГОРИТМ ПОЛУЧАЕТСЯ ОЧЕНЬ СЛОЖНЫЙ. И ЭТО ТОЛЬКО ПРЯМОЙ ПРОХОД! А КАК ЖЕ НЕСКОЛЬКО АЛГОРИТМОВ ОБУЧЕНИЯ?
А КАК ЖЕ ГРЯЗНЫЕ ХАКИ ДЛЯ РЕКУРРЕНТНЫХ СВЯЗЕЙ? А КАК ЖЕ СУМАСШЕДШИЙ OVERHEAD ОТ MPI??!!
ТАК ДЕЛАТЬ НЕЛЬЗЯ! ПАРАЛЛЕЛИТЬ СЕТИ НА НЕСКОЛЬКО МАШИН МОЖНО ТОЛЬКО ТОГДА, КОГДА ЭТО ФАКТИЧЕСКИ НЕЗАВИСИМЫЕ
БОЛЬШИЕ КУСКИ! ЭТО БЫВАЕТ ОЧЕНЬ РЕДКО, А НА ОБЫЧНЫХ СЕТЯХ ОТ ЭТОГО БУДУТ ТОЛЬКО ПРОБЛЕМЫ С ЗАДЕРЖКАМИ, ЛОКАМИ И Т.Д.

Составим новый алгоритм. Он должен быть простым и понятным, быть многопоточным, сеть должна быть полностью объектной.
MPI это конечно хорошо, но его можно будет задействовать в другом виде распараллеливания, а именно в пакетном 
обучении. А так - обойдемся потоками.

Итак.

поскольку все потоки работают в одном адресном пространстве, все будет намного проще.
Все нейроны поделены между рабочими потоками. Количество потоков определяет пользователь, но
информация о распределении нейронов между потоками "вшита" в структуру сети.

Представим себе, что входные нейроны уже наполнены входами.
1)смотрим, кто к ним подключен. удаляем дубликаты. Это можно сделать с пом. set<>
2)каждый рабочий поток берется за обработку своих нейронов.
2.1)поток поочередно для каждого нейрона считает локальное рецептивное поле и вычисляет значение активации
2.2)поток смотрит, кто подключен дальше к этому нейрону и добавляет эту информацию в set<>, для след. шага
2.3)добавляются только те нейроны, у которых hop count на 1 больше, чем у текущего нейрона.
это для устранения циклов в рекуррентных сетях
3)все продолжается по новой, только для нового слоя